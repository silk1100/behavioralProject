** DataDivisor
	* BUG TYPE: Coding-Technical-implementation

	DataDivisor main function
	*	divisor = DataDivisor(df, df_p, 'srs')
   	        df = divisor.divide()
     	        df = divisor.get_group('comm','mild')
		len(df) = 41
	
	Running Ipython in (D:\phd\codes\behavioralProject\data\feature_extraction\medianMinusPlus) while reading the phenotype file from 
	(D:\PhD\codes\behavioralProject\data)
	*	df = pd.read_csv('raw.csv', index_col='subj_id')
	*	comm = df['SRS_COMMUNICATION_T']
	*	comm_nona = comm.dropna()
	*	len(comm_nona[(comm_nona['SRS_COMMUNICATION_T']>=60)&(comm_nona['SRS_COMMUNICATION_T']<=65)]) = 50

	* Both values are expected to be the same.

	[SOLVED!!] In the first case the operation is made on the dataframee of the join between the data and the second one is running only on the phenotype 
		   file which has more subjects.

********************************************************************************************************************************************************************************
** DataDivisor
	* BUG TYPE: Research-Logical

	1. Not all of the subjects have conducted the whole SRS test. I assume that some of them are diagnosed once 2 or 3 tests pass the minimum total score
	   to be diagnosed as ASD. Therefore, the test is concluded and the subject is diagnosed.

	2. I have been using the overall diagnosis (TD/ASD) which is found in column "DX_GROUP" to classify between subjects instead of using a specific SRS
	   test score. The problem is that some of the subjects can score a moderate ASD score in a specific SRS module while they score TD in the rest modules
 	   Therefore, they get the overall diagnoses to be TD even though they posses some ASD traits. A more comprehensive experiment would be for each module
	   three classifiers are made (sever vs td), (moderate vs td), and (mild vs td) while totally ignoring the DX_Group index. A new subject is then passed to
	   the three classifiers and assigned to the class with highest probability.

********************************************************************************************************************************************************************************
** main_experiments.py
	* BUG TYPE: Coding-Technical-Implementation
	
	1. utils.save_model(os.path.join(self.stampfldr_, 'Xselected.p'), (Xselected, y)) # Doesnt work (never executed)
	2. utils.save_model(os.path.join(self.stampfldr_, 'normalizer.p'), normalizer) # Doesnt work (never executed)
	3. self._create_pseudo_scores(Xselected, y, ml_obj=self.ML_grid_) # Keeps crashing with weird errors like (ValueError: X has 34 features, but DecisionTreeRegressor is expecting 535 features as input)
	4. If one of the classifiers loaded to be utilized in feature selection doesn't have coef_ or feature_importances_, then the code crashes

	[SOLVED!!] The bug was deeper than what I've thought. It is related to Classifiers.py file. It overwrites the classifiers when they are written in the dictionary.
	           Therefore I get different # of input features for classifiers and it was a mess.


********************************************************************************************************************************************************************************
** main_experiments.py
	* BUG TYPE: Coding-Technical-Implementation
	1. Experiment class accepts only data from experiment_designer.py file and if you send to it a data file to process, it ignores the file and just load the one inside
	experiment_designer.py

********************************************************************************************************************************************************************************
** main_experiments.py/DataDivisor.py
	* BUG TYPE: Coding-Technical-Implementation-logic
	1. group_df_beforeFixation.csv always return only the severity level as chosen in experiment_designer.py exp_params['DD']['severity_group'] and it doesnt return any other
	groups such as TD. Therefore, there will always be group_df_afterFixation.csv regardless. (SOLVED)
	2. _create_pseudocode_scores() -> create an unfitted version of the classifier and repeat the train_test_split to have more confident results. (SOLVED)
	3. Xselected.p has a problem as it gives different results than when features are extracted from data using FSobj.
	
	
	


	
